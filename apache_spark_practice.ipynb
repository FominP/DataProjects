{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "# 1. Установка и запуск PySpark в Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "a28e17c9-9c90-4311-80a7-938da5624dbf"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [Conn\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "46 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Homework\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "tTUzkXm1ULML",
        "outputId": "e9f24679-e23d-46f3-a160-d5f5eb210e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e60f03d4d00>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://bd8983204946:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Homework</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Часть 1\n",
        "### 1. Создайте rdd в формате ключ значение."
      ],
      "metadata": {
        "id": "fMUxBKO-Uyjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([(\"apple\", 2), (\"banana\", 3), (\"apple\", 4), (\"cherry\", 1)])"
      ],
      "metadata": {
        "id": "GkW4fNC5T_r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Посчитайте среднее по ключу reduceByKey."
      ],
      "metadata": {
        "id": "g9ZD8obyUrXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rdd = (\n",
        "    rdd\n",
        "    .mapValues(lambda x: (x, 1))\n",
        "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "    .mapValues(lambda x: x[0] / x[1])\n",
        ")\n",
        "print(mean_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--y0TOFxUaYR",
        "outputId": "2e9f3eda-32d3-4f6d-c8fc-5ae31efc6feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('banana', 3.0), ('cherry', 1.0), ('apple', 3.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Посчитайте среднее по ключу через aggregateByKey."
      ],
      "metadata": {
        "id": "Sv2kPSjhU3EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rdd = (\n",
        "    rdd\n",
        "    .aggregateByKey((0, 0), lambda acc, value: (acc[0] + value, acc[1] + 1), lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "    .mapValues(lambda x: x[0] / x[1])\n",
        ")\n",
        "print(mean_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B49APZ05UE0e",
        "outputId": "a0438e49-7ddd-411a-9e8c-b0653dcadacc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('banana', 3.0), ('cherry', 1.0), ('apple', 3.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Посчитайте среднее по ключу через combineByKey."
      ],
      "metadata": {
        "id": "oTB81eiEU9zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rdd = (\n",
        "    rdd\n",
        "    .combineByKey(\n",
        "        lambda value: (value, 1),\n",
        "        lambda acc, value: (acc[0] + value, acc[1] + 1),\n",
        "        lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        "    )\n",
        "    .mapValues(lambda x: x[0] / x[1])\n",
        ")\n",
        "print(mean_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xAHLaKwVCX7",
        "outputId": "d39c8f22-7b3d-421b-c322-8bc55f23ecf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('banana', 3.0), ('cherry', 1.0), ('apple', 3.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Подумайте, как можно реализовать distinct с помощью существующих трансформаций."
      ],
      "metadata": {
        "id": "zqmHT0nyVC8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_rdd = rdd.reduceByKey(lambda a, b: a)\n",
        "print(distinct_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKP7R_9VVL08",
        "outputId": "e734c866-0840-436b-acbd-abcdf850f4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('banana', 3), ('cherry', 1), ('apple', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Часть 2 (с использованием RDD)\n",
        "### 1. Скачайте файл"
      ],
      "metadata": {
        "id": "vMJQ5C5NVmqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "path = \"https://raw.githubusercontent.com/deanwampler/spark-scala-tutorial/master/data/shakespeare/all-shakespeare.txt\"\n",
        "req = requests.get(path)\n",
        "url_content = req.content.decode(\"utf-8\").splitlines()\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    punctuation = r\"\"\".,\"!?:;\\'\"`\"\"\"\n",
        "    return ''.join(char.lower() for char in text if char not in punctuation).replace('\\t', ' ')\n",
        "\n",
        "url_content = [remove_punctuation(line) for line in url_content]\n",
        "\n",
        "text_rdd = sc.parallelize(url_content)\n",
        "print(text_rdd.take(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfC43N2Za6BY",
        "outputId": "b785eb09-f5be-4bb3-fb61-50f6168009ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' 1 king henry iv', '', '', ' dramatis personae', '', '', 'king henry the fourth (king henry iv)', '', '', 'henry']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Решите задачу wordcount для скаченного набора текста. Результат сохраните в текстовый в файл."
      ],
      "metadata": {
        "id": "xG8Ckf15V3rM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = (\n",
        "    text_rdd\n",
        "    .flatMap(lambda line: line.split())\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "    .sortBy(lambda x: x[1], ascending=False)\n",
        ")\n",
        "output_dir = \"wordcount_result\"\n",
        "word_counts.saveAsTextFile(output_dir)"
      ],
      "metadata": {
        "id": "ohcMWpH4WzLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/wordcount_result/part-00000', mode='r')\n",
        "lines = file.readlines()\n",
        "\n",
        "for line in lines[:10]:\n",
        "    print(line.strip())\n",
        "\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49Py7rq-eMZb",
        "outputId": "75632274-ea01-40de-ed0c-07d3d0b80083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('the', 30052)\n",
            "('and', 27512)\n",
            "('i', 21411)\n",
            "('to', 20726)\n",
            "('of', 18758)\n",
            "('a', 15950)\n",
            "('you', 13972)\n",
            "('my', 12953)\n",
            "('in', 11779)\n",
            "('that', 11513)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Решите задачу wordcount без стопслов (их можно либо скачать отдельно, либо использовать из библиотеки nltk) c применением broadcast. Результат сохраните в файл (parquet)."
      ],
      "metadata": {
        "id": "9BZaS7EZVtUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_counts = (\n",
        "    text_rdd\n",
        "    .flatMap(lambda line: [word.lower() for word in line.split() if word.lower() not in stop_words])\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "    .sortBy(lambda x: x[1], ascending=False)\n",
        ")\n",
        "\n",
        "output_dir = \"wordcount__stopwords_result\"\n",
        "word_counts.saveAsTextFile(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMQosOtpW2DZ",
        "outputId": "e927f5f0-06a1-4438-f674-338e3ffc8b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/wordcount__stopwords_result/part-00000', mode='r')\n",
        "lines = file.readlines()\n",
        "\n",
        "for line in lines[:10]:\n",
        "    print(line.strip())\n",
        "\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsLnb6yjfnrv",
        "outputId": "72156d4c-48be-4eb8-8d45-7ddf235875ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('thou', 5776)\n",
            "('thy', 4272)\n",
            "('shall', 3721)\n",
            "('thee', 3371)\n",
            "('king', 3287)\n",
            "('lord', 3279)\n",
            "('sir', 3004)\n",
            "('good', 2908)\n",
            "('well', 2610)\n",
            "('come', 2594)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. На основе задачи 3 постройте статистику частотности встречаемости слов, н/п 10 раз встречались слова [school, love и тд], формат вывода: число – список слов через ','"
      ],
      "metadata": {
        "id": "kw1elyIMWF0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_counts = (\n",
        "    text_rdd\n",
        "    .flatMap(lambda line: [word.lower() for word in line.split() if word.lower() not in stop_words])\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "    .sortBy(lambda x: x[1], ascending=False)\n",
        "    .collect()\n",
        ")\n",
        "\n",
        "words_by_count = {count: [word] for word, count in word_counts[:10]}\n",
        "for count, words in words_by_count.items():\n",
        "    if len(words) > 1:\n",
        "        words_by_count[count] = sorted(words)\n",
        "    print(f\"{count} - {words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdVYwAYLXCcV",
        "outputId": "8452f614-bd53-4d02-98f6-d23a92a87585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5776 - ['thou']\n",
            "4272 - ['thy']\n",
            "3721 - ['shall']\n",
            "3371 - ['thee']\n",
            "3287 - ['king']\n",
            "3279 - ['lord']\n",
            "3004 - ['sir']\n",
            "2908 - ['good']\n",
            "2610 - ['well']\n",
            "2594 - ['come']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Часть 3 (с использованием DataFrame/Spark SQL)\n",
        "### 1. Решите задачу wordcount для скаченного набора текста. Результат сохраните в текстовый в файл."
      ],
      "metadata": {
        "id": "vraNFSm1WMhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, desc, split, lower, when, explode\n",
        "df = spark.createDataFrame(url_content, StringType()).toDF(\"value\")\n",
        "wordcount_df = (df\n",
        "                .select(split(lower(col(\"value\")), \" \").alias(\"words\"))\n",
        "                .withColumn(\"word\", explode(\"words\"))\n",
        "                .groupBy(\"word\")\n",
        "                .agg(count(\"word\").alias(\"num\"))\n",
        "                .orderBy(desc(\"num\"))\n",
        "                )\n",
        "\n",
        "with open(\"wordcount_df.txt\", \"w\") as f:\n",
        "    f.write(str([(row.word, row.num) for row in wordcount_df.collect()]))"
      ],
      "metadata": {
        "id": "0q6vJORNXJbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcount_df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JI5T__B9DrX",
        "outputId": "6606f070-2935-477b-a76a-42a3341b0d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|word|   num|\n",
            "+----+------+\n",
            "|    |151275|\n",
            "| the| 30052|\n",
            "| and| 27512|\n",
            "|   i| 21411|\n",
            "|  to| 20726|\n",
            "|  of| 18758|\n",
            "|   a| 15950|\n",
            "| you| 13972|\n",
            "|  my| 12953|\n",
            "|  in| 11779|\n",
            "+----+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Решите задачу wordcount без стопслов (их можно либо скачать отдельно, либо использовать из библиотеки nltk) c применением broadcast. Результат сохраните в файл (parquet).\n"
      ],
      "metadata": {
        "id": "W4bzElTxWajq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, desc, split, lower, when, explode\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "df = spark.createDataFrame(url_content, StringType()).toDF(\"value\")\n",
        "wordcount_df = (df\n",
        "                .select(split(lower(col(\"value\")), \" \").alias(\"words\"))\n",
        "                .withColumn(\"word\", explode(\"words\"))\n",
        "                .filter(~col(\"word\").isin(stop_words))\n",
        "                .groupBy(\"word\")\n",
        "                .agg(count(\"word\").alias(\"num\"))\n",
        "                .orderBy(desc(\"num\"))\n",
        "                )\n",
        "\n",
        "wordcount_df.write.parquet(\"wordcount_df.parquet\")"
      ],
      "metadata": {
        "id": "KzEAW5PdXPis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet('/content/wordcount_df.parquet')\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktD5KdgRIf0M",
        "outputId": "4eeb89e6-c0c1-4306-e6d8-4d4e4aceb67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   word     num\n",
            "0                        151275\n",
            "1                  thou    5776\n",
            "2                   thy    4272\n",
            "3                 shall    3721\n",
            "4                  thee    3371\n",
            "...                 ...     ...\n",
            "32364              hewd       1\n",
            "32365        saddle-bow       1\n",
            "32366         servilely       1\n",
            "32367         cupbearer       1\n",
            "32368  behind-door-work       1\n",
            "\n",
            "[32369 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. На основе задачи 3 постройте статистику частотности встречаемости слов, н/п 10 раз встречались слова [school, love и тд], формат вывода: число – список слов через ','"
      ],
      "metadata": {
        "id": "NWx-18p3WgHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import collect_list, sort_array, size\n",
        "\n",
        "words_by_count = (\n",
        "    wordcount_df\n",
        "    .groupBy('num')\n",
        "    .agg(\n",
        "        sort_array(collect_list('word')).alias('words')\n",
        "    )\n",
        "    .withColumn('word_count', size('words'))\n",
        "    .sort('num', ascending=False)\n",
        ")\n",
        "\n",
        "words_by_count = {count: [word] for word, count in word_counts[:10]}\n",
        "for count, words in words_by_count.items():\n",
        "    if len(words) > 1:\n",
        "        words_by_count[count] = sorted(words)\n",
        "    print(f\"{count} - {words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPNLf8QVXSp4",
        "outputId": "beacb0c7-96cd-444b-d99d-853347da18c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5776 - ['thou']\n",
            "4272 - ['thy']\n",
            "3721 - ['shall']\n",
            "3371 - ['thee']\n",
            "3287 - ['king']\n",
            "3279 - ['lord']\n",
            "3004 - ['sir']\n",
            "2908 - ['good']\n",
            "2610 - ['well']\n",
            "2594 - ['come']\n"
          ]
        }
      ]
    }
  ]
}